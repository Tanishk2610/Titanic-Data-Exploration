{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f14118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbd0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1bda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.uniform(-1, 1, (hidden_size, input_size))\n",
    "    b1 = np.random.uniform(-1, 1, (hidden_size, 1))\n",
    "    W2 = np.random.uniform(-1, 1, (output_size, hidden_size))\n",
    "    b2 = np.random.uniform(-1, 1, (output_size, 1))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ef6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdc846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, A2):\n",
    "    m = Y.shape[1]\n",
    "    loss = -1/m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d145fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, A1, A2, W1, W2, b1, b2):\n",
    "    m = Y.shape[1]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80912268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25264cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, hidden_size, output_size, learning_rate, num_epochs):\n",
    "    input_size = X.shape[0]\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = compute_loss(Y, A2)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W1, W2, b1, b2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13fc9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    predictions = np.round(A2)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764ec93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.749097661743568\n",
      "Epoch 100, Loss: 0.7142572283900298\n",
      "Epoch 200, Loss: 0.7002204606275971\n",
      "Epoch 300, Loss: 0.6947210592039549\n",
      "Epoch 400, Loss: 0.6925676503169234\n",
      "Epoch 500, Loss: 0.6916996597050943\n",
      "Epoch 600, Loss: 0.6913212329273984\n",
      "Epoch 700, Loss: 0.691128525483351\n",
      "Epoch 800, Loss: 0.6910060888644972\n",
      "Epoch 900, Loss: 0.6909101284379104\n",
      "Epoch 1000, Loss: 0.690824025951917\n",
      "Epoch 1100, Loss: 0.6907414603783852\n",
      "Epoch 1200, Loss: 0.6906600122195115\n",
      "Epoch 1300, Loss: 0.6905787410699606\n",
      "Epoch 1400, Loss: 0.6904972678629103\n",
      "Epoch 1500, Loss: 0.6904154269280811\n",
      "Epoch 1600, Loss: 0.6903331339261642\n",
      "Epoch 1700, Loss: 0.6902503356335131\n",
      "Epoch 1800, Loss: 0.690166990796441\n",
      "Epoch 1900, Loss: 0.6900830627985083\n",
      "Epoch 2000, Loss: 0.6899985168297458\n",
      "Epoch 2100, Loss: 0.6899133187774698\n",
      "Epoch 2200, Loss: 0.6898274347788711\n",
      "Epoch 2300, Loss: 0.6897408310300118\n",
      "Epoch 2400, Loss: 0.6896534736954024\n",
      "Epoch 2500, Loss: 0.6895653288577888\n",
      "Epoch 2600, Loss: 0.6894763624844967\n",
      "Epoch 2700, Loss: 0.6893865404008954\n",
      "Epoch 2800, Loss: 0.6892958282671205\n",
      "Epoch 2900, Loss: 0.6892041915564153\n",
      "Epoch 3000, Loss: 0.6891115955343539\n",
      "Epoch 3100, Loss: 0.6890180052385972\n",
      "Epoch 3200, Loss: 0.6889233854589982\n",
      "Epoch 3300, Loss: 0.6888277007179535\n",
      "Epoch 3400, Loss: 0.6887309152509329\n",
      "Epoch 3500, Loss: 0.6886329929871486\n",
      "Epoch 3600, Loss: 0.6885338975303261\n",
      "Epoch 3700, Loss: 0.6884335921395514\n",
      "Epoch 3800, Loss: 0.6883320397101746\n",
      "Epoch 3900, Loss: 0.6882292027547486\n",
      "Epoch 4000, Loss: 0.6881250433839856\n",
      "Epoch 4100, Loss: 0.6880195232877169\n",
      "Epoch 4200, Loss: 0.6879126037158402\n",
      "Epoch 4300, Loss: 0.6878042454592392\n",
      "Epoch 4400, Loss: 0.6876944088306678\n",
      "Epoch 4500, Loss: 0.6875830536455774\n",
      "Epoch 4600, Loss: 0.6874701392028855\n",
      "Epoch 4700, Loss: 0.6873556242656693\n",
      "Epoch 4800, Loss: 0.6872394670417746\n",
      "Epoch 4900, Loss: 0.6871216251643317\n",
      "Epoch 5000, Loss: 0.6870020556721693\n",
      "Epoch 5100, Loss: 0.6868807149901182\n",
      "Epoch 5200, Loss: 0.6867575589091962\n",
      "Epoch 5300, Loss: 0.686632542566671\n",
      "Epoch 5400, Loss: 0.6865056204259918\n",
      "Epoch 5500, Loss: 0.6863767462565853\n",
      "Epoch 5600, Loss: 0.6862458731135143\n",
      "Epoch 5700, Loss: 0.686112953316994\n",
      "Epoch 5800, Loss: 0.6859779384317621\n",
      "Epoch 5900, Loss: 0.6858407792463055\n",
      "Epoch 6000, Loss: 0.6857014257519396\n",
      "Epoch 6100, Loss: 0.6855598271217443\n",
      "Epoch 6200, Loss: 0.6854159316893559\n",
      "Epoch 6300, Loss: 0.6852696869276205\n",
      "Epoch 6400, Loss: 0.6851210394271151\n",
      "Epoch 6500, Loss: 0.6849699348745371\n",
      "Epoch 6600, Loss: 0.6848163180309779\n",
      "Epoch 6700, Loss: 0.6846601327100834\n",
      "Epoch 6800, Loss: 0.6845013217561178\n",
      "Epoch 6900, Loss: 0.6843398270219397\n",
      "Epoch 7000, Loss: 0.6841755893469099\n",
      "Epoch 7100, Loss: 0.6840085485347445\n",
      "Epoch 7200, Loss: 0.6838386433313371\n",
      "Epoch 7300, Loss: 0.6836658114025682\n",
      "Epoch 7400, Loss: 0.6834899893121289\n",
      "Epoch 7500, Loss: 0.6833111124993878\n",
      "Epoch 7600, Loss: 0.6831291152573263\n",
      "Epoch 7700, Loss: 0.6829439307105839\n",
      "Epoch 7800, Loss: 0.682755490793641\n",
      "Epoch 7900, Loss: 0.6825637262291877\n",
      "Epoch 8000, Loss: 0.6823685665067173\n",
      "Epoch 8100, Loss: 0.6821699398613951\n",
      "Epoch 8200, Loss: 0.6819677732532518\n",
      "Epoch 8300, Loss: 0.6817619923467598\n",
      "Epoch 8400, Loss: 0.6815525214908533\n",
      "Epoch 8500, Loss: 0.6813392836994536\n",
      "Epoch 8600, Loss: 0.6811222006325751\n",
      "Epoch 8700, Loss: 0.6809011925780833\n",
      "Epoch 8800, Loss: 0.6806761784341875\n",
      "Epoch 8900, Loss: 0.680447075692753\n",
      "Epoch 9000, Loss: 0.6802138004235267\n",
      "Epoch 9100, Loss: 0.6799762672593728\n",
      "Epoch 9200, Loss: 0.6797343893826222\n",
      "Epoch 9300, Loss: 0.6794880785126493\n",
      "Epoch 9400, Loss: 0.6792372448947891\n",
      "Epoch 9500, Loss: 0.6789817972907235\n",
      "Epoch 9600, Loss: 0.6787216429704633\n",
      "Epoch 9700, Loss: 0.6784566877060679\n",
      "Epoch 9800, Loss: 0.6781868357672455\n",
      "Epoch 9900, Loss: 0.677911989918987\n",
      "Predictions: [[0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "W1, b1, W2, b2 = train(X, Y, hidden_size, output_size, learning_rate, num_epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(X, W1, b1, W2, b2)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
